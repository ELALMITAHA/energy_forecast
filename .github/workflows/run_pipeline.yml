name: Daily MLOps Forecast Pipeline

on:
  schedule:
    - cron: "0 1 * * *"
  workflow_dispatch:

jobs:
  run_pipeline:
    runs-on: ubuntu-latest

    steps:
      # 1. Checkout repository
      - name: Checkout repository
        uses: actions/checkout@v4

      # 2. Setup Python
      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.12"

      # 3. Install dependencies
      - name: Install Python dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt

      # 4. Install HuggingFace CLI
      - name: Install HuggingFace CLI
        run: |
          curl -LsSf https://hf.co/cli/install.sh | bash

      # 5. Prepare artifact directories
      - name: Prepare artifact directories
        run: |
          mkdir -p ${{ github.workspace }}/tmp/data/raw
          mkdir -p ${{ github.workspace }}/tmp/data/processed
          mkdir -p ${{ github.workspace }}/tmp/artifacts/forecasts
          mkdir -p ${{ github.workspace }}/tmp/artifacts/models
          mkdir -p ${{ github.workspace }}/tmp/artifacts/data_quality
          mkdir -p ${{ github.workspace }}/tmp/artifacts/logs

      # 6. Run Dagster Job
      - name: Run Dagster Job
        env:
          RAW_DIR: ${{ github.workspace }}/tmp/data/raw
          PROCESSED_DIR: ${{ github.workspace }}/tmp/data/processed
          PROCESSED_FINAL_DIR: ${{ github.workspace }}/tmp/data/processed/final
          FORECAST_DIR: ${{ github.workspace }}/tmp/artifacts/forecasts
          METRICS_DIR: ${{ github.workspace }}/tmp/artifacts/metrics
          MODELS_DIR: ${{ github.workspace }}/tmp/artifacts/models
          DATA_QUALITY_DIR: ${{ github.workspace }}/tmp/artifacts/data_quality
          LOGS_DIR: ${{ github.workspace }}/tmp/artifacts/logs
        run: |
          dagster job execute \
            -f src/flow/run_full_pipeline_dagster.py \
            -j full_pipeline \
            --config executor_config.yaml

      # 7. Copy generated artifacts to upload folder
      - name: Copy generated artifacts
        run: |
          mkdir -p data-artifact/models
          mkdir -p data-artifact/forecasts
          mkdir -p data-artifact/data_quality
          cp ${{ github.workspace }}/tmp/artifacts/models/*.pkl data-artifact/models/ || true
          cp ${{ github.workspace }}/tmp/artifacts/forecasts/*.parquet data-artifact/forecasts/ || true
          cp ${{ github.workspace }}/tmp/artifacts/data_quality/*.json data-artifact/data_quality/ || true
          cp ${{ github.workspace }}/tmp/artifacts/logs/*.log data-artifact/logs/ || true

      # 8. Upload artifacts to HuggingFace dataset in subfolders
      - name: Upload artifacts to HuggingFace dataset
        env:
          HF_TOKEN: ${{ secrets.HF_TOKEN }}
        run: |
          hf upload tahaelalmi/energy-forecast-artifacts/models \
            data-artifact/models \
            --repo-type dataset \
            --token $HF_TOKEN

          hf upload tahaelalmi/energy-forecast-artifacts/forecasts \
            data-artifact/forecasts \
            --repo-type dataset \
            --token $HF_TOKEN

          hf upload tahaelalmi/energy-forecast-artifacts/data_quality \
            data-artifact/data_quality \
            --repo-type dataset \
            --token $HF_TOKEN

      # 9. Upload logs as GitHub Action artifact
      - name: Upload pipeline logs
        uses: actions/upload-artifact@v4
        with:
          name: pipeline-log
          path: data-artifact/logs/









